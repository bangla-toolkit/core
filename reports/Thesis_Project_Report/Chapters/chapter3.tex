\chapter{Methodology}

This chapter describes the systematic and structured process employed in the development of Byakaron. It highlights the processes, methodologies, and instruments used for the purpose of fulfilling the research objectives. The methodology offers a clear roadmap for the development process, ensuring reliability and replicability of the work.

\section{Research Design}
The Byakaron project is based on an applied research process involving empirical data gathering using software engineering methods. The project uses a hybrid approach which combines:

\begin{enumerate}
    \item \textbf{Statistical Methods}: N-gram models of language to compute probability-based word correction
    \item \textbf{Rule-Based Methods}: Explicit correction rules for known error patterns
    \item \textbf{Database-Driven Design}: Structured storage of language data for efficient querying
\end{enumerate}

This hybrid approach combines the strengths of n-gram in its statistical robustness models \cite{khan2014ngram, pal2021ngram} and the accuracy of rule-based systems in dealing with well-defined error patterns.

\section{System Architecture}

\subsection{Monorepo Structure}
The code is organized in a monorepo fashion using Bun workspaces, allowing code sharing and dependency management across multiple packages. The architecture is depicted in Figure \ref{fig:sys_arch}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{Visuals/system_architecture.png}
    \caption{Overall System Architecture of Byakaron showing the data flow from sources through processing to the application layer.}
    \label{fig:sys_arch}
\end{figure}

The directory structure includes:
\begin{itemize}
    \item \texttt{apps/}: User-facing applications
    \begin{itemize}
        \item \texttt{byakoron}: Main web interface (Next.js)
        \item \texttt{training}: Training scripts for model training
    \end{itemize}
    \item \texttt{packages/}: Shared libraries
    \begin{itemize}
        \item \texttt{core}: Core NLP algorithms
        \item \texttt{db}: Database schema and connection logic
        \item \texttt{dataset}: ETL pipelines for data processing
    \end{itemize}
    \item \texttt{reports/}: Thesis documentation
    \item \texttt{docs/}: Project documentation (Docusaurus)
\end{itemize}

\subsection{Data Flow Architecture}
The system uses a pipeline architecture in which the data passes from raw sources through various processing steps:

\begin{enumerate}
    \item \textbf{Extract}: Raw texts are downloaded from data sources such as Wikipedia dumps and news websites
    \item \textbf{Transform}: The text is cleaned, tokenized, and converted into structured formats
    \item \textbf{Load}: The processed data is efficiently loaded into PostgreSQL
    \item \textbf{Serve}: The web application queries the database in order to supply correction suggestions for the user's input
\end{enumerate}

The data flow process, as shown by Figure \ref{fig:data_pipeline}, continues through the Byakaron engine.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Visuals/data_pipeline.png}
    \caption{Data Pipeline illustrating the flow of data from Bangla Wikipedia to NLP processing to the database and correction engine.}
    \label{fig:data_pipeline}
\end{figure}

\section{Data Model Design}

\subsection{Entity-Relationship Design}
The database schema is designed to enable statistical spell checking via n-gram models and rule-based corrections. In Figure \ref{fig:er_diagram}, the entity-relationship diagram is shown.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Visuals/er_diagram.png}
    \caption{Entity Relationship Diagram illustrating the database schema for Byakaron.}
    \label{fig:er_diagram}
\end{figure}

The detailed schema with word pairs and grammar rules is presented in Figure \ref{fig:er_detailed}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{Visuals/er_diagram_detailed.png}
    \caption{Detailed schema of the database, highlighting relations between words, word\_pairs, rules, and grammar\_rules tables.}
    \label{fig:er_detailed}
\end{figure}

\subsection{Core Tables}
The Prisma ORM implementation of the database schema consists of the following important tables:

\subsubsection{Sentences Table}
Stores raw sentences extracted from data sources:
\begin{lstlisting}[language=SQL, basicstyle=\footnotesize\ttfamily, breaklines=true]
sentence (
    id          Int         PRIMARY KEY
    text        Text        NOT NULL
    created_at  Timestamp   DEFAULT now()
)
\end{lstlisting}

\subsubsection{Words Table}
Contains unique Bangla words with their properties:
\begin{lstlisting}[language=SQL, basicstyle=\footnotesize\ttfamily, breaklines=true]
words (
    id          Int         PRIMARY KEY
    value       Text        UNIQUE NOT NULL
)
\end{lstlisting}

\subsubsection{Word Pairs Table}
The statistical spell checking engine's skeleton, holding the frequencies of bigrams:
\begin{lstlisting}[language=SQL, basicstyle=\footnotesize\ttfamily, breaklines=true]
word_pairs (
    id          Int         PRIMARY KEY
    prev_id     Int         REFERENCES words(id)
    next_id     Int         REFERENCES words(id)
    weight      Float       NOT NULL
    occurance   Int         NOT NULL
)
\end{lstlisting}

The \texttt{word\_pairs} table facilitates calculations of probabilities for bigrams. For a word sequence $w_1, w_2$, the probability of $w_2$ given $w_1$ is calculated as:
\begin{equation}
    P(w_2 | w_1) = \frac{Count(w_1, w_2)}{Count(w_1)}
    \label{eq:bigram}
\end{equation}

This bigram model is useful in picking out the improbable sequences of words that could reveal misspelling errors.

\subsubsection{Grammar Rules Table}
Stores explicit correction rules:
\begin{lstlisting}[language=SQL, basicstyle=\footnotesize\ttfamily, breaklines=true]
grammar_rules (
    id              Int     PRIMARY KEY
    type            Varchar NOT NULL
    description     Text
)

rules (
    id                  Int     PRIMARY KEY
    keyword             Text    NOT NULL
    replace_keyword     Text    NOT NULL
    error_description   Text
    grammar_rule_id     Int     REFERENCES grammar_rules(id)
)
\end{lstlisting}

\section{Core NLP Algorithms}

\subsection{Tokenization}
The tokenization module breaks down text into sentences and words, simultaneously performing cleanup operations. This is shown in Figure \ref{fig:nlp_modules}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{Visuals/nlp_modules.png}
    \caption{NLP Processing Modules: Normalization, Tokenization, POS and NER Tagging.}
    \label{fig:nlp_modules}
\end{figure}

The key tokenization steps are:
\begin{enumerate}
    \item \textbf{Sentence Splitting}: Splits text into sentences based on Bangla sentence-ending characters (\bn{ред}, ?, !)
    \item \textbf{Word Tokenization}: Breaks down a sentence into words
    \item \textbf{Text Cleanup}: Removes URLs, brackets, and other non-Bangla characters, as well as significant whitespace
\end{enumerate}

The cleanup operation uses regular expressions to segregate the pure Bangla text:
\begin{itemize}
    \item Removal of URLs and web links
    \item Removal of English characters and numbers
    \item Erasure of brackets and the material within them
    \item Normalization of Unicode characters
\end{itemize}

\subsection{Spell Checking Algorithm}
The spell checking engine uses a multi-step process, which is depicted in Figure \ref{fig:spell_engine}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Visuals/spell_engine_flow.png}
    \caption{Spell Checker Engine depicting the flow of Bangla words from tokenization to correction suggestions.}
    \label{fig:spell_engine}
\end{figure}

The spell checking procedure involves the following stages:

\subsubsection{Stage 1: Exact Match}
First, it checks for exact word matches with the dictionary, or words table. The words that have exact matches are regarded as spelled correctly.

\subsubsection{Stage 2: Soundex Match}
For words which are not found in the dictionary, a phonetic matching algorithm is used to identify similarly sounding words. This is especially important for the Bangla language as there are many similarly pronounced letters.

\subsubsection{Stage 3: Fuzzy Match}
In case of no soundex match, fuzzy string matching by edit distance reveals potential corrections. The edit distance $d(a, b)$ from two words is defined as:
\begin{equation}
    d(a, b) = \min(insertions + deletions + substitutions)
    \label{eq:edit_distance}
\end{equation}

Suggestions based on words within the edit distance are displayed based on their frequency in the corpus.

\subsubsection{Stage 4: N-gram Context}
Lastly, the bigram model obtained from \texttt{word\_pairs} is employed to rank suggestions on contextual probability. Given candidate corrections $C = \{c_1, c_2, ..., c_n\}$ for a word following word $w_{prev}$:
\begin{equation}
    best\_candidate = \arg\max_{c \in C} P(c | w_{prev})
    \label{eq:context_ranking}
\end{equation}

\section{Data Acquisition and Processing}

\subsection{Data Sources}
The main sources for the data used in the corpus are listed in Figure \ref{fig:data_sources}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{Visuals/data_sources.png}
    \caption{Data Sources: Wikipedia dumps and major Bangla news websites.}
    \label{fig:data_sources}
\end{figure}

The main data source is the Bengali Wikipedia dump (\texttt{bnwiki-latest-pages-articles.xml.bz2}) \cite{wikidumps2024}, which provides:
\begin{itemize}
    \item Clean, edited, and refereed text
    \item Broad vocabulary coverage for topics
    \item Writing quality consistency
\end{itemize}

\subsection{ETL Pipeline}
The ETL process is implemented in the \texttt{packages/dataset} package:

\subsubsection{Extract Phase}
\begin{itemize}
    \item Download Wikipedia XML files
    \item XML parsing for article extraction
    \item Strip MediaWiki markup and templates
\end{itemize}

\subsubsection{Transform Phase}
\begin{itemize}
    \item Clean text using regex-based rules
    \item Tokenization for sentences and words
    \item Word frequency analysis
    \item Calculate bigram statistics for words
    \item Apply normalization rules
\end{itemize}

\subsubsection{Load Phase}
\begin{itemize}
    \item Bulk insert using \texttt{pg-copy-streams} for efficiency
    \item Create indexes to support quick querying
    \item Data integrity validation
\end{itemize}

\section{Technology Stack}
The various choices of technology in the project are summarized in Table \ref{tab:tech_stack}.

\begin{table}[ht]
    \centering
    \caption{Technology stack used for Byakaron development.}
    \begin{tabular}{l l l}
        \toprule
        \textbf{Component} & \textbf{Technology} & \textbf{Purpose} \\
        \toprule
        Runtime & Bun & Fast JavaScript/TypeScript runtime \\
        \midrule
        Language & TypeScript & Type-safe development \\
        \midrule
        Database & PostgreSQL & Relational data storage \\
        \midrule
        ORM & Prisma & Database schema and queries \\
        \midrule
        Frontend & Next.js 14+ & Web application framework \\
        \midrule
        Styling & Tailwind CSS & Utility-first CSS framework \\
        \bottomrule
    \end{tabular}
    \label{tab:tech_stack}
\end{table}

\section{Ethical Considerations}
The project follows all the ethics in data collection and processing:

\begin{itemize}
    \item \textbf{Data Sources}: The data is all sourced from publicly available sources such as Wikipedia under Creative Commons licenses
    \item \textbf{Open Source}: The software is developed on the principles of open source, which helps to encourage transparency and community contribution
    \item \textbf{Privacy}: There is no collection or storage of personal data from the user by the application
    \item \textbf{Accessibility}: This web application has been developed to be accessible to people with different levels of technical skill
\end{itemize}

\section{Limitations}
The drawbacks of the proposed methodology are the following:

\begin{itemize}
    \item \textbf{Data Bias}: Wikipedia might not capture colloquial Bangla language usage
    \item \textbf{Computational Resources}: Massive data processing demands extensive processing power
    \item \textbf{Grammar Coverage}: Grammar checking using grammar rules has low coverage compared with machine learning methods
    \item \textbf{Domain Specificity}: The machine learning model might behave differently for different domains (academic writing, creative writing, technical writing)
\end{itemize}


\chapter{Literature Review}

This report will cover and compile the latest research in the area of Bangla spell checking and grammar correction, and place it in the broader field of Natural Language Processing (NLP) research. It will provide a backdrop to the research and identify the gaps and trends in the existing knowledge base, which underpins the Byakaron system.

\section{Bangla Natural Language Processing: Overview}
Bangla (Bengali) is the seventh most commonly spoken language in the world, with a native speaker base of over 300 million people. However, in NLP-based applications, Bangla remains a ``low-resource'' language due to a lack of appropriately labeled data resources and complete models of the language itself \cite{sarker2021bnlp}. In the latter few years, there has been increasing interest in NLP applications in the Bangla language from academia and also from the open-source community. The BNLP toolkit provides basic support in tokenization, stemming, POS-tagging, and named entity identification tasks in the Bangla language itself \cite{sarker2021bnlp}. The same goes for the OpenBangla effort with input method editors and basic spell-checking support \cite{openbangla2020}.

\section{Spell Checking Methods}

\subsection{Traditional Methods}
Initially, spell-checking systems for Bangla relied upon simple dictionary searches and a measure of edit distance. Khan et al. (2014) developed a spell-checking system that used N-gram models to check the correctness of a Bangla word, with a detection accuracy of 96.17\% for their test data \cite{khan2014ngram}. This experiment indicated that a character-level n-gram model can identify spelling errors, even without knowing a complete dictionary of words.

Pal et al. (2021) advanced a bit upon previous ideas by proposing an automated spelling correction system with a combination of a tri-gram model and neighboring words, yielding a higher detection ratio compared to a simple model involving edit distance \cite{pal2021ngram}.

Chowdhury and Akhter (2020) developed a system involving a combination of a context-sensitive model and an n-gram language model to check spelling and punctuation, yielding a high accuracy of 87.58\% with a large Bangla dataset \cite{chowdhury2020context}.

\subsection{Deep Learning}
The transformer era made a dramatic change in spell checking in other languages too. Jahin et al. (2023) developed the spell checker system ``BSpell'' specifically for the Bangla language that combined CNNs and the BERT pre-training in both word and character-levels to handle the complex morphology of Bangla language and OOV words more effectively \cite{hasan2023bspell}.

Hossain (2024) proposed ``DPCSpell,'' which used denoising transformers to design a detector-purificator-corrector system that could remove only incorrect characters and achieve better results than the state-of-the-art spell checking model in the Bangla language \cite{hossain2024dpcspell}.

\section{Grammar Correction Research}

\subsection{Rule-Based Systems}
Conventional grammar correctors for the Bangla language are based upon the application of human-annotated rules expressing linguistic knowledge. Descriptions of patterns for typical grammatical errors and their corrections are specified. Though accurate for identified error patterns, the rule-based approach has poor coverage and requires significant human expertise for extension \cite{avroproject2003}.

\subsection{Transformer-Based Grammar Correction}
Miah et al. investigated the use of the T5 model in grammatical error detection, fine-tuning a BanglaT5 model on 9,385 sentences to demonstrate the efficiency of transformer models in obtaining a small Levenshtein distance in error detection, while the need to post-process to obtain the best results cannot be overemphasized \cite{miah2023t5, bhattacharjee2022banglat5}.

Alam et al. modelled grammatical error detection as a token classification task and applied sequence labeling using BanglaBERT to identify grammatical, punctuation, and spelling errors jointly, allowing more extensive correctibilities \cite{bhattacharjee2022banglabert, alam2024tokenclassification}.

\subsection{Large Language Models for Grammar Correction}
Islam et al. (2024) analyzed the application of Large Language Models for Grammatical Error Correction in the Bangla language \cite{islam2024gec}. They identified 12 types of errors and designed a noise injection approach based on rules to synthesize the dataset. This enhanced the accuracy for Grammatical Error Correction by 3 to 7\% after tuning the instruction LLMs on the synthesized dataset, close to the accuracy of the human evaluation for the identification of errors.

\section{Benchmark Data}

\subsection{VAIYAKARANA Benchmark}
The VAIYAKARANA system was developed by Bhattacharyya \& Bhattacharya (2024) \cite{bhattacharyya2024vaiyakarana}. It was a benchmark for grammar correction in the Bangla language to meet the demand for a standardized metric in Bangla GEC. Key characteristics include:

\begin{itemize}
    \item Classification of errors into 5 categories and 12 sub-categories
    \item 92,830 grammatically incorrect sentences and 18,426 correct sentences
    \item An extended version of 567,422 sentences and 227,119 error samples
    \item 619 human-written sentences from native-written essays
\end{itemize}

This benchmark offers a systematic approach to creating incorrect sentences, which can also be used for other Indian languages. This can be helpful in dealing with the lack of annotated data in low-resource languages.

\subsection{Wikipedia-Based Corpora}
Dump files from Wikipedia have been an integral source for the Bangla language corpus. The Bengali version of the Wikipedia database, available through Wikimedia data dumps \cite{wikidumps2024}, has over 169,000 articles that can be used for NLP applications. Data processing pipes involve the removal of Mediawiki codes through the use of WikiExtractor software \cite{abujar2019dataset}.

\section{Existing Tools and Platforms}

\subsection{OpenBangla Keyboard}
The OpenBangla Keyboard \cite{openbangla2020} is an open-source, Unicode-compliant input method for GNU/Linux systems with spell checking capabilities. It provides:
\begin{itemize}
    \item Dictionary-based suggestions for phonetically similar words
    \item Autocorrect for commonly misspelled frequently used words
    \item Integration of common English words in a Bangla context
\end{itemize}

Although OpenBangla is a helpful input assistant tool, its spell checking capabilities are restricted to dictionary lookup and context-free correction.

\subsection{Avro Keyboard}
The Avro Keyboard \cite{avroproject2003}, developed by OmicronLab, was the first to offer phonetic typing functionality for Bangla and came equipped with a spell checker. Although Avro is a popular software, Avro's spell checking uses a static dictionary method that does not involve statistical and/or contextual analysis.

\subsection{Commercial Solutions}
There are commercial alternatives such as REVE NLP, which offers Bangla spell and grammar checks as part of their enterprise solutions. However, such solutions are not available for free use or research, thus their overall impact on the open-source community is limited.

\section{Technology Foundations}

\subsection{Transformer Architecture}
The Transformer model, which was introduced by Vaswani et al. in 2017, has become the foundation for modern NLP systems \cite{vaswani2017attention}. The self-attention mechanism allows models designed to capture long-range dependencies within text, making them particularly useful in language understanding tasks.

\subsection{BERT and Language Models}
BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2019bert} introduced the concept of bidirectional pretraining, enabling the model to understand context from both directions. BanglaBERT \cite{bhattacharjee2022banglabert} further extends this to Bangla, providing pre-trained representations learnable to fine-tune for downstream tasks.

\subsection{T5 and Text-to-Text Framework}
The T5 model \cite{raffel2020t5} treats all NLP tasks as text-to-text tasks, allowing for a unified approach to various applications such as grammar correction. BanglaT5 \cite{bhattacharjee2022banglat5} adapts this framework for Bangla, providing a foundation for sequence-to-sequence tasks.

\section{Research Gap Analysis}
Although there have been major developments in NLP for the Bangla language, some gaps remain:

\begin{enumerate}
    \item \textbf{Integrated Solutions}: All the related work done so far is either on spell checking or grammar correction in isolation. There is a lack of integrated systems that address both of these issues together.
    
    \item \textbf{Accessible Tools}: Though there are prototypes, there are not many production-ready, open-source tools that exist for practical application purposes.
    
    \item \textbf{Dataset Availability}: Despite the existence of benchmark datasets like VAIYAKARANA, processed datasets that provide word frequency information and n-gram models appropriate for statistical analysis are relatively scarce.
    
    \item \textbf{Hybrid Approaches}: The literature indicates that hybrid approaches involving statistical methods with rule-based corrections may have the potential to increase the accuracy, but few implementations demonstrate this effectively.
    
    \item \textbf{Web-Based Accessibility}: The majority of tools are command-line based or require installation, which restricts accessibility for general users.
\end{enumerate}

\section{Positioning of Byakaron}
The Byakaron project fills this gap through:

\begin{itemize}
    \item Development of a hybrid model that incorporates N-gram statistical models, such as bigram word pairs, with explicit rule-based corrections
    \item Offering processed Wikipedia data in terms of sentence corpora, word dictionaries, and word pair frequency statistics
    \item Developing reusable NLP tools for tokenization, normalization, and text processing
    \item Developing an accessible web application using Next.js and TypeScript
    \item Contributing to the Bangla community of open-source NLP with well-documented, modular code
\end{itemize}

This positioning strategy enables Byakaron to fill the gaps while leveraging the strong foundation established by previous research in the field.
